{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are going to look at Dynamic Recurrent Neural Networks.\n",
    "\n",
    "Dynamic Recurrent Neural Networks are unique becuase they allow for an arbitrary length in value. Now the value can be arbitry but we must have a fixed max length because we will pad them with zeros. A real world example might be video which is not necessariyl always going to be shot the same length. In the example we are going to use toy data to simulate the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the code we are going to generate a sequence of data. It will have two types of class of sequences with dynamic length. One will a linear sequence and the other will be a random sequence. For both sequences of dynamic length we are going to pad them with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# toy random data generator\n",
    "class ToySequenceData(object):\n",
    "    \"\"\"\n",
    "    Generate squence of data with dynamic length.\n",
    "    Geenrates samples for training:\n",
    "    - Class 0: linear seq [0, 1, 2, 3, 4...]\n",
    "    - Class 1: Random Seq [1, 3, 10, ,7...]\n",
    "\n",
    "    Notice: we must pad each seq with zeros\n",
    "    to the max seq len to keep the consistency.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3,\n",
    "              max_value=1000):\n",
    "    self.data = []\n",
    "    self.labels = []\n",
    "    self.seqlen = []\n",
    "    for i in range(n_samples):\n",
    "        # random seq length\n",
    "        len = random.randint(min_seq_len, max_seq_len)\n",
    "        # monitor sequence length int sequence (50% prob)\n",
    "        if random.random() < .5:\n",
    "            # generate a linear sequence\n",
    "            rand_start = random.randint(0, max_value - len)\n",
    "            s = [[float(i)/max_value] for i in\n",
    "                range(rand_start, rand_start + len)]\n",
    "            # pad seq for dimensions consistency\n",
    "            s += [[0.] for i in range(max_seq_len - len)]\n",
    "            self.data.append(s)\n",
    "            self.labels.append([1., 0.])\n",
    "        else:\n",
    "            # generat a ramdom seq\n",
    "            s = [[float(random.randint(0, max_value))/max_value]\n",
    "                for i in range(len)]\n",
    "            # pad seq for dimensions consistency\n",
    "            s += [[0.] for i in range(max_seq_len - len)]\n",
    "            self.data.append(s)\n",
    "            self.labels.append([0., 1.])\n",
    "    self.batch_id = 0\n",
    "    \n",
    "    def next(self, batch_size):\n",
    "        \"\"\" \n",
    "          Return a batch of data. when dataser end is reached,\n",
    "          start over.\n",
    "        \"\"\"\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "        batch_data = (self.data[self.batch_id:min(self.batch +\n",
    "                                                 batch_size, len(self.data))])\n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id +\n",
    "                                                     batch_size, len(self.data))])\n",
    "        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\n",
    "                                                     batch_size, len(self.data))])\n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        return batch_data, batch_labels, batch_seqlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a method of generating our data we can now begin building the model. First lets define our hyper parameters and our network parameters. After wards, we will obtain the training data and test data. Now we are ready to begin defining the tensor. We will need a x & y tensor as well as a sequence length. Finally, we can define the weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper paramaters\n",
    "training_rate = 0.01\n",
    "training_iters = 100000\n",
    "batch_size = 10\n",
    "display_step = 10\n",
    "\n",
    "# network parameters\n",
    "seq_max_len = 20\n",
    "n_hidden = 64\n",
    "n_classes = 2\n",
    "\n",
    "trainset = ToySequenceData(n_samples=1000, max_seq_len=seq_max_len)\n",
    "testset = ToySequenceData(n_samples=500, max_seq_len=seq_max_len)\n",
    "\n",
    "# The TF graph\n",
    "x = tf.placeholder(\"float\", [None, seq_max_len, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# the weights\n",
    "\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to design the dynamic RNN. We are going to reshape \n",
    "the data to match the RNN requirements. After we reshape we are going to\n",
    "define the lstm cell. Now that we have the lstm we can perform the\n",
    "dynamic calculation. When we perform a dynamic calculation we need to \n",
    "retrieve the last output. To retain the last ouputs we must \n",
    "build an index system. Next we pass the data through an activation op.\n",
    "\n",
    "Now that we have defined the dynamic RNN we can initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def dynamicRNN(x, seqlen, weights, biases):\n",
    "  \n",
    "    # prepare data shape to match 'rnn' function requirements\n",
    "    # current data input shape: (batch_size, n_steps, n_input)\n",
    "    # required shape: n_steps tensor list of shape (batch_size, n_input)\n",
    "\n",
    "    # permutating batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, 1])\n",
    "    # split to get a list of n_steps tensor of shape (batch_size, n_input)\n",
    "    x = tf.split(x, seq_max_len, 0)\n",
    "\n",
    "    # define a lstm cell with TF\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # get lstm cell output, providing sequence_length withh perform\n",
    "    # dynamic calculation\n",
    "    outputs, states = lstm(lstm_cell, x, dtype=tf.float32,\n",
    "                             sequence_length=seqlen)\n",
    "    # when performing dynamic calculation, we must retrieve the last\n",
    "    # dynamic computer output. If a sequence length is 10,\n",
    "    # we need to retrieve the 10 output\n",
    "\n",
    "    # However TF doesn't support advanced indexing yet, so we build\n",
    "    # ac sutom op that for each sample in batch size, get its length\n",
    "    # and get the corresponding relevant output\n",
    "\n",
    "    # outputs is a lost of putput at every timestep, we pack\n",
    "    # them in a tensor and change back dimensions to\n",
    "    # [batch_size, n_steps, n_input]\n",
    "    outputs = tf.pack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # hack to build the index and retrieval\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # start index for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    # linear activation using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']\n",
    "\n",
    "pred = dynamicRNN(x, seqlen, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to define some functions such as the cost and optimization function. In this example we are going to reduce the mean and calculate the logits with softmax cross entropy. For the optimizer portion of the model we are going to use gradient descent with our hyper parameter learning rate. Now that we have those function defined we will define a function witch lets us now if our correction is correct. Finally, we define we are going to determine the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define lost and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.GradientDescent(learning_rate=learning_rate)\n",
    "\n",
    "# evaluate the model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# initialize the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the neural network!\n",
    "\n",
    "First, we are going to start the tesorflow session. We are going to iterate through the number of iteration we defined in our hyper parameters. Next, we get the data in batches with batch of x, batch of y and the length of the batch. We are now ready to feed the data to our optimizer function where we do backpropagation. Now we can display information every see often so we can now the accuracy of and the loss score over time as we train. Training is now complete.\n",
    "\n",
    "Before we finish it is always a great idea to evaluate our train neural network. In order to make sure our neural network performs well we are going to test it with data it has not seen before. We get the data, label and sequence length to feed to the network. Once we get the output we can analyse how accurate our dynamic recurrent neural network is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y, batch_seqlen = trainst.next(batch_size)\n",
    "        # run optimizer op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                      seqlen: batch_seqlen})\n",
    "        if step % display_step == 0:\n",
    "            # calc accuracy\n",
    "            acc = ses.run(accuracy, feed_dict={x: batch_x, y: batch_y,\n",
    "                                            seqlen: batch_seqlen})\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y,\n",
    "                                            seqlen: batch_seqlen})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print \"Opt complete\"\n",
    "\n",
    "    # calculate accuracy\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n",
    "                                      seqlen: test_seqlen}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
