{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are going to buid a recurrent neural network using the MNIST data set of hand written digits. This standard neural neural network are ideal to use when we need a sense of tie or sequence. The RNN keeps working weights which get updated over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check to see where the mnist gets defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define the hyper parameters and the network parameters. After wards, we define the graph and the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 10000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# network parameters\n",
    "n_input = 28 # MNIST data input\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer number of features\n",
    "n_classes = 10 # total classes\n",
    "\n",
    "# graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# define the weights\n",
    "weights = { 'out' : tf.Variable(tf.random_normal([n_hidden, n_classes])) }\n",
    "biases = { 'out' : tf.Variable(tf.random_normal([n_classes])) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the recurrent neural network. The RNN needs the parameters of x, weights and biases. We are going to prepare the data to create a basic LSTM cell. Now that we have the created LSTM cell we can get the outputs. Once we have the outputs we can now multiple the previous output of the cell with the weights of the current state and add a the bias weight. Now let's use our shinny new LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the RNN\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    x = tf.transpose(x, [1, 0, 2])  # Permuting batch_size and n_steps\n",
    "    x = tf.reshape(x, [-1, n_input])    # reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.split(x, n_steps, 0)       # split to get a list of n_steps tensor of shape (batch_size, n_input)\n",
    "    \n",
    "    # define a lstm cell w/ TF\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    \n",
    "    # get lstm cell output\n",
    "    outputs, states = lstm(lstm_cell, x)\n",
    "    \n",
    "    # linear activation, using rnn ineeer loop last output\n",
    "    return tf.matmul(outputs[-1]. weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to define the cost function. The cost\n",
    "function in this example will reduce the mean while it does the softmax cross entropy with logits. The optimizer we are going to use is the AdamOptimizer with our learning rate we defined in our hyper parameters. We then minimize the cost by invoking the minimize function. We can now initilize all the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learnibng_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# intialize the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have defined everything we need to start the training. We are going to start the tensorflow sesion and we are going to loop through the number of iterations we set in our hyper parameters. First, we will get the data in batches with the batch size we defined in our hyper parameter section. We will then reshape the tensor to get a 28 sequence of 28 elements. Once that is ready we can now feed it to our optimizer where it will calculate the backpropagation. In the next step, we will pring every so often to see how well the network is doing while training. We will calculate the accuracy and the loss score and print it out to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    \n",
    "    while step * batch_size < training_iters:\n",
    "        # get batched training data\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # reshape the data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        \n",
    "        # run opt op aka backprop\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "            # calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            print \"Iter \"+ str(step*batch_size) + \", Minibatch Loss = \" + \\\n",
    "                \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
