{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to focus on a Bi-Directional Recurrent Neural Network. This type of recurrent neural network is unique from others becasue instead of keeping recurrent in a forward way it also keeps record of a backwards way. Essentially looking into the future and also looking at the past examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to get a few things started before we dive deep with deep learning. Before we start we need get the input data and define a few paramaters such as: hyper paramaters, network paramaters, define the weights and define the TF graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data-sets/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../data-sets/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data-sets/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data-sets/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../data-sets/MNIST/\", one_hot=True)\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# network parameters\n",
    "n_input = 28\n",
    "n_steps = 28\n",
    "n_hidden = 128\n",
    "n_classes = 10\n",
    "\n",
    "# defince weights & biases\n",
    "weights = {'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "# define the graph\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets define the Bi-Directional Recurant Neural Network. This RNN cell requires a forward cell and a backwards cell which we will define using the Basic LSTM Cell function. After we define the LSTM cells we must pass it to static bi-directional rnn which will take into account both. Once we get the output we multiple the outputs with the weights and add the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "    \"\"\"Prepare data and bidirectiona RNN\n",
    "    Current data: (batch_size, n_steps, n_input)\n",
    "    required data: 'n_steps' tensor list of shape (batch_size, n_input)\n",
    "    \"\"\"\n",
    "    # permutating batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # reshape to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # split to get a list of n_steps tensor of shape (batch_size, n_input)\n",
    "    x = tf.split(x, n_steps, 0)\n",
    "\n",
    "    # define lstm cells with tensorflow\n",
    "    # forward deriction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    # backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                                dtype=tf.float32)\n",
    "    except Exception:\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                          dtype=tf.float32)\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = BiRNN(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the code we are going to define a few critical componets of a neural network. We are going to define the cost function and the optimizer function. The cost function in this example is in charge of reducing the mean by doing a softmax on the cross entropy with logits. We are also going to define function to evaluate the neural network. We will define a funttion that let us not wether the prediction is correct and we are also going to define the accurace of the model. After we defin our function we are going to initilize our varibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# initializing the varibales\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally start the training. \n",
    "\n",
    "In this section of the code we are going to start the tensorflow session and begin training with batches of the data. First we loop through the number of training interations we want train for. Once in the loop we can get the MNIST data, hand writted digits, in batches. Next, we need to reshape the data to 28 sequences of 28 elements becuase the size of the image is 28x28. Now we are ready to optimize so we feed the image and the answer. \n",
    "\n",
    "In the following step we will keep track of our accuracy and loss score. We print to the console every so often we don't flood it with information.\n",
    "\n",
    "We are finallly done trainin the neural network! But now we should probably test the model. To test the model we will feed it never before seen examples and grade how will the neural network performs. We define the number of images we want to test as well the test data and the label data. Now we finally done with it all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 1.951641, Training Accuracy= 1.95164\n",
      "Iter 2560, Minibatch Loss= 1.633784, Training Accuracy= 1.63378\n",
      "Iter 3840, Minibatch Loss= 1.312482, Training Accuracy= 1.31248\n",
      "Iter 5120, Minibatch Loss= 1.095966, Training Accuracy= 1.09597\n",
      "Iter 6400, Minibatch Loss= 0.891447, Training Accuracy= 0.89145\n",
      "Iter 7680, Minibatch Loss= 1.256486, Training Accuracy= 1.25649\n",
      "Iter 8960, Minibatch Loss= 0.889787, Training Accuracy= 0.88979\n",
      "Iter 10240, Minibatch Loss= 0.741981, Training Accuracy= 0.74198\n",
      "Iter 11520, Minibatch Loss= 0.496239, Training Accuracy= 0.49624\n",
      "Iter 12800, Minibatch Loss= 0.869694, Training Accuracy= 0.86969\n",
      "Iter 14080, Minibatch Loss= 0.672333, Training Accuracy= 0.67233\n",
      "Iter 15360, Minibatch Loss= 0.408935, Training Accuracy= 0.40894\n",
      "Iter 16640, Minibatch Loss= 0.523268, Training Accuracy= 0.52327\n",
      "Iter 17920, Minibatch Loss= 0.284163, Training Accuracy= 0.28416\n",
      "Iter 19200, Minibatch Loss= 0.280324, Training Accuracy= 0.28032\n",
      "Iter 20480, Minibatch Loss= 0.181882, Training Accuracy= 0.18188\n",
      "Iter 21760, Minibatch Loss= 0.435711, Training Accuracy= 0.43571\n",
      "Iter 23040, Minibatch Loss= 0.171901, Training Accuracy= 0.17190\n",
      "Iter 24320, Minibatch Loss= 0.435467, Training Accuracy= 0.43547\n",
      "Iter 25600, Minibatch Loss= 0.420926, Training Accuracy= 0.42093\n",
      "Iter 26880, Minibatch Loss= 0.259987, Training Accuracy= 0.25999\n",
      "Iter 28160, Minibatch Loss= 0.253304, Training Accuracy= 0.25330\n",
      "Iter 29440, Minibatch Loss= 0.338995, Training Accuracy= 0.33899\n",
      "Iter 30720, Minibatch Loss= 0.334957, Training Accuracy= 0.33496\n",
      "Iter 32000, Minibatch Loss= 0.203251, Training Accuracy= 0.20325\n",
      "Iter 33280, Minibatch Loss= 0.290257, Training Accuracy= 0.29026\n",
      "Iter 34560, Minibatch Loss= 0.241953, Training Accuracy= 0.24195\n",
      "Iter 35840, Minibatch Loss= 0.228187, Training Accuracy= 0.22819\n",
      "Iter 37120, Minibatch Loss= 0.324197, Training Accuracy= 0.32420\n",
      "Iter 38400, Minibatch Loss= 0.185779, Training Accuracy= 0.18578\n",
      "Iter 39680, Minibatch Loss= 0.186335, Training Accuracy= 0.18633\n",
      "Iter 40960, Minibatch Loss= 0.349318, Training Accuracy= 0.34932\n",
      "Iter 42240, Minibatch Loss= 0.111818, Training Accuracy= 0.11182\n",
      "Iter 43520, Minibatch Loss= 0.181989, Training Accuracy= 0.18199\n",
      "Iter 44800, Minibatch Loss= 0.240031, Training Accuracy= 0.24003\n",
      "Iter 46080, Minibatch Loss= 0.139562, Training Accuracy= 0.13956\n",
      "Iter 47360, Minibatch Loss= 0.292896, Training Accuracy= 0.29290\n",
      "Iter 48640, Minibatch Loss= 0.251374, Training Accuracy= 0.25137\n",
      "Iter 49920, Minibatch Loss= 0.273445, Training Accuracy= 0.27344\n",
      "Iter 51200, Minibatch Loss= 0.137669, Training Accuracy= 0.13767\n",
      "Iter 52480, Minibatch Loss= 0.174521, Training Accuracy= 0.17452\n",
      "Iter 53760, Minibatch Loss= 0.042893, Training Accuracy= 0.04289\n",
      "Iter 55040, Minibatch Loss= 0.204201, Training Accuracy= 0.20420\n",
      "Iter 56320, Minibatch Loss= 0.159041, Training Accuracy= 0.15904\n",
      "Iter 57600, Minibatch Loss= 0.166737, Training Accuracy= 0.16674\n",
      "Iter 58880, Minibatch Loss= 0.208197, Training Accuracy= 0.20820\n",
      "Iter 60160, Minibatch Loss= 0.122065, Training Accuracy= 0.12207\n",
      "Iter 61440, Minibatch Loss= 0.178546, Training Accuracy= 0.17855\n",
      "Iter 62720, Minibatch Loss= 0.051004, Training Accuracy= 0.05100\n",
      "Iter 64000, Minibatch Loss= 0.141285, Training Accuracy= 0.14129\n",
      "Iter 65280, Minibatch Loss= 0.122060, Training Accuracy= 0.12206\n",
      "Iter 66560, Minibatch Loss= 0.104647, Training Accuracy= 0.10465\n",
      "Iter 67840, Minibatch Loss= 0.097169, Training Accuracy= 0.09717\n",
      "Iter 69120, Minibatch Loss= 0.110645, Training Accuracy= 0.11064\n",
      "Iter 70400, Minibatch Loss= 0.044538, Training Accuracy= 0.04454\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    while step * batch_size < training_iters:\n",
    "        # lets get the data in batches\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # reshape the data to 28 seq of 28 elements to run optimization\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        if step % display_step == 0:\n",
    "            # calculate batch accuracy & loss\n",
    "            acc = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "  \n",
    "    # calculate accuracy for 128 test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print \"Testing Accuracy: \", sess.run(accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
