{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to focus on a Bi-Directional Recurrent Neural Network. This type of recurrent neural network is unique from others becasue instead of keeping recurrent in a forward way it also keeps record of a backwards way. Essentially looking into the future and also looking at the past examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to get a few things started before we dive deep with deep learning. Before we start we need get the input data and define a few paramaters such as: hyper paramaters, network paramaters, define the weights and define the TF graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../data-sets/MNIST/\", one_hot=True)\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# network parameters\n",
    "n_input = 28\n",
    "n_steps = 28\n",
    "n_hidden = 128\n",
    "n_classes = 10\n",
    "\n",
    "# defince weights & biases\n",
    "weights = {'out': tf.Varibale(tf.random_normal([2*n_hidden, n_classes]))}\n",
    "biases = {'out': tf.Varibale(tf.random_normal([n_classes]))}\n",
    "\n",
    "# define the graph\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets define the Bi-Directional Recurant Neural Network. This RNN cell requires a forward cell and a backwards cell which we will define using the Basic LSTM Cell function. After we define the LSTM cells we must pass it to static bi-directional rnn which will take into account both. Once we get the output we multiple the outputs with the weights and add the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "    \"\"\"Prepare data and bidirectiona RNN\n",
    "    Current data: (batch_size, n_steps, n_input)\n",
    "    required data: 'n_steps' tensor list of shape (batch_size, n_input)\n",
    "    \"\"\"\n",
    "    # permutating batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # reshape to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # split to get a list of n_steps tensor of shape (batch_size, n_input)\n",
    "    x = tf.split(x, n_steps, 0)\n",
    "\n",
    "    # define lstm cells with tensorflow\n",
    "    # forward deriction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    # backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                                dtype=tf.float32)\n",
    "    except Exception:\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                          dtype=tf.float32)\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = BiRNN(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the code we are going to define a few critical componets of a neural network. We are going to define the cost function and the optimizer function. The cost function in this example is in charge of reducing the mean by doing a softmax on the cross entropy with logits. We are also going to define function to evaluate the neural network. We will define a funttion that let us not wether the prediction is correct and we are also going to define the accurace of the model. After we defin our function we are going to initilize our varibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, lanels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# initializing the varibales\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally start the training. \n",
    "\n",
    "In this section of the code we are going to start the tensorflow session and begin training with batches of the data. First we loop through the number of training interations we want train for. Once in the loop we can get the MNIST data, hand writted digits, in batches. Next, we need to reshape the data to 28 sequences of 28 elements becuase the size of the image is 28x28. Now we are ready to optimize so we feed the image and the answer. \n",
    "\n",
    "In the following step we will keep track of our accuracy and loss score. We print to the console every so often we don't flood it with information.\n",
    "\n",
    "We are finallly done trainin the neural network! But now we should probably test the model. To test the model we will feed it never before seen examples and grade how will the neural network performs. We define the number of images we want to test as well the test data and the label data. Now we finally done with it all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    while step * batch_size < training_iters:\n",
    "        # lets get the data in batches\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # reshape the data to 28 seq of 28 elements to run optimization\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        if step % display_step == 0:\n",
    "            # calculate batch accuracy & loss\n",
    "            acc = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "  \n",
    "    # calculate accuracy for 128 test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print \"Testing Accuracy: \", sess.run(accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
