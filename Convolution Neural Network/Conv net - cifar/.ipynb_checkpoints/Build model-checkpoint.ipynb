{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Basic model parameters.\n",
    "tf.app.flags.DEFINE_integer('batch_size', 128,\n",
    "                            \"\"\"Number of images to process in a batch.\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_dir', '/tmp/cifar10_data',\n",
    "                           \"\"\"Path to the CIFAR-10 data directory.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('use_fp16', False,\n",
    "                            \"\"\"Train the model using fp16.\"\"\")\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "IMAGE_SIZE = cifar10_input.IMAGE_SIZE\n",
    "NUM_CLASSES = cifar10_input.NUM_CLASSES\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.1       # Initial learning rate.\n",
    "\n",
    "# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n",
    "# to differentiate the operations. Note that this prefix is removed from the\n",
    "# names of the summaries when visualizing a model.\n",
    "TOWER_NAME = 'tower'\n",
    "\n",
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "    \"\"\"Helper to create summaries for activations\"\"\"\n",
    "    \n",
    "    # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "    # session. This helps the clarity of presentation on tensorboard.\n",
    "    tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity',\n",
    "                                       tf.nn.zero_fraction(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_on_cpu(name, shape, initilizer):\n",
    "    \"\"\"Helper to create a Variable stored on CPU memory.\"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        dtype = tf.float if FLAG.use_fp16 else tf.float32\n",
    "        var = tf.get_variable(name, shape, initiliazer=initializer, dtype=dtype)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    \"\"\"Helper to create an initialized Variable with weight decay.\"\"\"\n",
    "    dtype = tf.float15 if FLAG.use_fp16 else tf.float32\n",
    "    var = _varible_on_cpu(\n",
    "        name, shape, \n",
    "        tf.truncated_normal_initilizer(stddev=stddev, dtype=dtype))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distorted_inputs():\n",
    "    \"\"\"Construct distorted input for CIFAR training using the Reader ops.\"\"\"\n",
    "    if not FLAGS.data_dir:\n",
    "        raise ValueError('Please supply a data_dir')\n",
    "    data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n",
    "    images, labels = cifar10_input.distorted_inputs(data_dir=data_dir,\n",
    "                                                   batch_size=FLAGS.batch_size)\n",
    "    if FLAGS.use_fp16:\n",
    "        images = tf.cast(images, tf.float16)\n",
    "        labels = tf.cast(labels, tf.float16)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inputs(eval_data):\n",
    "    if not FLAGS.data_dirs:\n",
    "        raise ValueError('Please supply a data_dir')\n",
    "    data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n",
    "    images, labels, = cifar10_input.inputs(eval_data=eval_data,\n",
    "                                          data_dir=data_dir,\n",
    "                                          batch_size=FLAGS.batch_size)\n",
    "    if FLAG.use_fp16:\n",
    "        images = tf.cast(images, tf.float16)\n",
    "        labels = tf.cast(labels, tf.float16)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(image):\n",
    "    \"\"\"Build the CIFAR-10 model.\"\"\"\n",
    "    # We instantiate all variables using tf.get_variable() instead of\n",
    "    # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "    # If we only ran this model on a single GPU, we could simplify this function\n",
    "    # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "    \n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernal = _variable_with_weight_decay('weights',\n",
    "                                            shape[5, 5, 3, 64],\n",
    "                                            stddev=5e-2,\n",
    "                                            wd=0.0)\n",
    "        conv = tf.nn.conv2d(images, kernal, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv1)\n",
    "        \n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                          padding='SAME', name='pool1')\n",
    "    #norm 1\n",
    "    norm - tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.01 / 9.0, beta=0.75,\n",
    "                    name='norm1')\n",
    "    # conv2 \n",
    "    with tf.variable_scope('conv2') as scoope:\n",
    "        kernal = _variable_with_weight_decay('weights',\n",
    "                                            shape=[5, 5, 64, 64],\n",
    "                                            stddev=5e-2,\n",
    "                                            wd=0.0)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv2)\n",
    "    \n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                     name='norm2')\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "    \n",
    "    # local 3\n",
    "    with tf.variable_scaope('local3') as scope:\n",
    "        #move everthing into depth so we can perform a single matrix multiply\n",
    "        reshape = tf.reshape(pool2, [FLAG.batch_size, -1])\n",
    "        dim = rehape.get_shape()[1].value\n",
    "        weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                             stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "        _activation_summary(local3)\n",
    "        \n",
    "    # local 4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = _variable_with_wdeight_decay('weights', shape=[384, 192],\n",
    "                                              stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "        _activation_summary(local4)\n",
    "        \n",
    "    # linear layer(WX + b),\n",
    "    # We don't apply softmax here because\n",
    "    # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "    # and performs the softmax internally for efficiency.\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = _variable_with_weight_decay('weight', [192, NUM_CLASSES],\n",
    "                                             stddev=1/192.0, wd=0.0)\n",
    "        biases = _variable_on_cpu('biases', [NUM_CLASSES],\n",
    "                                 tf.constant_initializer(0.0))\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "        _activation_summary(softmax_linear)\n",
    "        \n",
    "    return softmax_linear\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    \"\"\"Add L2Loss to all the trainable variables.\"\"\"\n",
    "    labels = tf.cast(label, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    \n",
    "    # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "    # decay terms (L2 loss).\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss):\n",
    "    \"\"\"Add summaries for losses in CIFAR-10 model.\"\"\"\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_average_op = loss_averages.apply(losses + [total_loss])\n",
    "    \n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "        # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "        # as the original loss name.\n",
    "        tf.summary.scalar(l.op.name + ' (raw)', 1)\n",
    "        tf.summary.scalar(l.op.name, loss_averages.average(1))\n",
    "        \n",
    "    return loss_averages_op\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-2271c605e5bb>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-2271c605e5bb>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    grads = opt.compute_gradients(total)loss)\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train(total_loss, global_steps):\n",
    "    \"\"\"Create an optimizer and apply to all trainable variables. Add moving\n",
    "  average for all trainable variables.\"\"\"\n",
    "    \n",
    "    # Variables that affect learning rate.\n",
    "    num_batches_per_epoch = NUM_EXAMPLE_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n",
    "    decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "    \n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(INITIAL_LERARNING_RATE,\n",
    "                                   global_step,\n",
    "                                   decay_step,\n",
    "                                   LEARNING_RATE_DECAY_FACTOR,\n",
    "                                   staircase=True)\n",
    "    \n",
    "    tf.summary.scaler('learning_rate', lr)\n",
    "    \n",
    "    # Generate moving averages of all losses and associated summaries.\n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "    \n",
    "    #compute gradients\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        opt = tf.train.GradientDescentOptimizer(lr)\n",
    "        grads = opt.compute_gradients(total_loss)\n",
    "        \n",
    "        #apply gradiensts\n",
    "        apply_gradient_op = opt.apply_gradients(grad, global_step=gloabal_step)\n",
    "        \n",
    "        # add histo\n",
    "        for var in tf.trainable._variables():\n",
    "            tf.sammary.histogram(var.op.name, var)\n",
    "            \n",
    "        # add hostogram\n",
    "        for grad, var in grads:\n",
    "            if grad is not None:\n",
    "                tf.suammary.histogram(var.op.name + '/gradients', gad)\n",
    "        \n",
    "        # Track the moving averages of all trainable variables.\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(\n",
    "            MOVING_AVERAGE_DECAY, global_step)\n",
    "        variabels_averages_op = variable_averages.apply(tf.trainnable_variables())\n",
    "        \n",
    "        with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "            \n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
